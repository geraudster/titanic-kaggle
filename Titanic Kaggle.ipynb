{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train <- read.csv('kaggle/train.csv', stringsAsFactors = FALSE)\n",
    "test <- read.csv('kaggle/test.csv', stringsAsFactors = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PassengerId</th><th scope=col>Survived</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>6</td><td>0</td><td>3</td><td>Moran, Mr. James</td><td>male</td><td>NA</td><td>0</td><td>0</td><td>330877</td><td>8.4583</td><td></td><td>Q</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & PassengerId & Survived & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked\\\\\n",
       "\\hline\n",
       "\t1 & 1 & 0 & 3 & Braund, Mr. Owen Harris & male & 22 & 1 & 0 & A/5 21171 & 7.25 &  & S\\\\\n",
       "\t2 & 2 & 1 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599 & 71.2833 & C85 & C\\\\\n",
       "\t3 & 3 & 1 & 3 & Heikkinen, Miss. Laina & female & 26 & 0 & 0 & STON/O2. 3101282 & 7.925 &  & S\\\\\n",
       "\t4 & 4 & 1 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel) & female & 35 & 1 & 0 & 113803 & 53.1 & C123 & S\\\\\n",
       "\t5 & 5 & 0 & 3 & Allen, Mr. William Henry & male & 35 & 0 & 0 & 373450 & 8.05 &  & S\\\\\n",
       "\t6 & 6 & 0 & 3 & Moran, Mr. James & male & NA & 0 & 0 & 330877 & 8.4583 &  & Q\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  PassengerId Survived Pclass\n",
       "1           1        0      3\n",
       "2           2        1      1\n",
       "3           3        1      3\n",
       "4           4        1      1\n",
       "5           5        0      3\n",
       "6           6        0      3\n",
       "                                                 Name    Sex Age SibSp Parch\n",
       "1                             Braund, Mr. Owen Harris   male  22     1     0\n",
       "2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n",
       "3                              Heikkinen, Miss. Laina female  26     0     0\n",
       "4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n",
       "5                            Allen, Mr. William Henry   male  35     0     0\n",
       "6                                    Moran, Mr. James   male  NA     0     0\n",
       "            Ticket    Fare Cabin Embarked\n",
       "1        A/5 21171  7.2500              S\n",
       "2         PC 17599 71.2833   C85        C\n",
       "3 STON/O2. 3101282  7.9250              S\n",
       "4           113803 53.1000  C123        S\n",
       "5           373450  8.0500              S\n",
       "6           330877  8.4583              Q"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PassengerId</th><th scope=col>Survived</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>6</td><td>0</td><td>3</td><td>Moran, Mr. James</td><td>male</td><td>NA</td><td>0</td><td>0</td><td>330877</td><td>8.4583</td><td></td><td>Q</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & PassengerId & Survived & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked\\\\\n",
       "\\hline\n",
       "\t1 & 1 & 0 & 3 & Braund, Mr. Owen Harris & male & 22 & 1 & 0 & A/5 21171 & 7.25 &  & S\\\\\n",
       "\t2 & 2 & 1 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599 & 71.2833 & C85 & C\\\\\n",
       "\t3 & 3 & 1 & 3 & Heikkinen, Miss. Laina & female & 26 & 0 & 0 & STON/O2. 3101282 & 7.925 &  & S\\\\\n",
       "\t4 & 4 & 1 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel) & female & 35 & 1 & 0 & 113803 & 53.1 & C123 & S\\\\\n",
       "\t5 & 5 & 0 & 3 & Allen, Mr. William Henry & male & 35 & 0 & 0 & 373450 & 8.05 &  & S\\\\\n",
       "\t6 & 6 & 0 & 3 & Moran, Mr. James & male & NA & 0 & 0 & 330877 & 8.4583 &  & Q\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  PassengerId Survived Pclass\n",
       "1           1        0      3\n",
       "2           2        1      1\n",
       "3           3        1      3\n",
       "4           4        1      1\n",
       "5           5        0      3\n",
       "6           6        0      3\n",
       "                                                 Name    Sex Age SibSp Parch\n",
       "1                             Braund, Mr. Owen Harris   male  22     1     0\n",
       "2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n",
       "3                              Heikkinen, Miss. Laina female  26     0     0\n",
       "4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n",
       "5                            Allen, Mr. William Henry   male  35     0     0\n",
       "6                                    Moran, Mr. James   male  NA     0     0\n",
       "            Ticket    Fare Cabin Embarked\n",
       "1        A/5 21171  7.2500              S\n",
       "2         PC 17599 71.2833   C85        C\n",
       "3 STON/O2. 3101282  7.9250              S\n",
       "4           113803 53.1000  C123        S\n",
       "5           373450  8.0500              S\n",
       "6           330877  8.4583              Q"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train$Survived <- factor(train$Survived)\n",
    "head(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    }
   ],
   "source": [
    "library(caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Created from 714 samples and 8 variables\n",
       "\n",
       "Pre-processing:\n",
       "  - bagged tree imputation (6)\n",
       "  - ignored (2)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PassengerId</th><th scope=col>Survived</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>6</td><td>0</td><td>3</td><td>Moran, Mr. James</td><td>male</td><td>31.87982</td><td>0</td><td>0</td><td>330877</td><td>8.4583</td><td></td><td>Q</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & PassengerId & Survived & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked\\\\\n",
       "\\hline\n",
       "\t1 & 1 & 0 & 3 & Braund, Mr. Owen Harris & male & 22 & 1 & 0 & A/5 21171 & 7.25 &  & S\\\\\n",
       "\t2 & 2 & 1 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599 & 71.2833 & C85 & C\\\\\n",
       "\t3 & 3 & 1 & 3 & Heikkinen, Miss. Laina & female & 26 & 0 & 0 & STON/O2. 3101282 & 7.925 &  & S\\\\\n",
       "\t4 & 4 & 1 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel) & female & 35 & 1 & 0 & 113803 & 53.1 & C123 & S\\\\\n",
       "\t5 & 5 & 0 & 3 & Allen, Mr. William Henry & male & 35 & 0 & 0 & 373450 & 8.05 &  & S\\\\\n",
       "\t6 & 6 & 0 & 3 & Moran, Mr. James & male & 31.87982 & 0 & 0 & 330877 & 8.4583 &  & Q\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  PassengerId Survived Pclass\n",
       "1           1        0      3\n",
       "2           2        1      1\n",
       "3           3        1      3\n",
       "4           4        1      1\n",
       "5           5        0      3\n",
       "6           6        0      3\n",
       "                                                 Name    Sex      Age SibSp\n",
       "1                             Braund, Mr. Owen Harris   male 22.00000     1\n",
       "2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.00000     1\n",
       "3                              Heikkinen, Miss. Laina female 26.00000     0\n",
       "4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.00000     1\n",
       "5                            Allen, Mr. William Henry   male 35.00000     0\n",
       "6                                    Moran, Mr. James   male 31.87982     0\n",
       "  Parch           Ticket    Fare Cabin Embarked\n",
       "1     0        A/5 21171  7.2500              S\n",
       "2     0         PC 17599 71.2833   C85        C\n",
       "3     0 STON/O2. 3101282  7.9250              S\n",
       "4     0           113803 53.1000  C123        S\n",
       "5     0           373450  8.0500              S\n",
       "6     0           330877  8.4583              Q"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcData <- preProcess(train[,-c(2,4,9,11)], method = c('bagImpute'))\n",
    "preProcData\n",
    "trainImpute <- predict(preProcData, train)\n",
    "head(trainImpute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PassengerId</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>892</td><td>3</td><td>Kelly, Mr. James</td><td>male</td><td>34.5</td><td>0</td><td>0</td><td>330911</td><td>7.8292</td><td></td><td>Q</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>893</td><td>3</td><td>Wilkes, Mrs. James (Ellen Needs)</td><td>female</td><td>47</td><td>1</td><td>0</td><td>363272</td><td>7</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>894</td><td>2</td><td>Myles, Mr. Thomas Francis</td><td>male</td><td>62</td><td>0</td><td>0</td><td>240276</td><td>9.6875</td><td></td><td>Q</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>895</td><td>3</td><td>Wirz, Mr. Albert</td><td>male</td><td>27</td><td>0</td><td>0</td><td>315154</td><td>8.6625</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>896</td><td>3</td><td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td><td>female</td><td>22</td><td>1</td><td>1</td><td>3101298</td><td>12.2875</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>897</td><td>3</td><td>Svensson, Mr. Johan Cervin</td><td>male</td><td>14</td><td>0</td><td>0</td><td>7538</td><td>9.225</td><td></td><td>S</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & PassengerId & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked\\\\\n",
       "\\hline\n",
       "\t1 & 892 & 3 & Kelly, Mr. James & male & 34.5 & 0 & 0 & 330911 & 7.8292 &  & Q\\\\\n",
       "\t2 & 893 & 3 & Wilkes, Mrs. James (Ellen Needs) & female & 47 & 1 & 0 & 363272 & 7 &  & S\\\\\n",
       "\t3 & 894 & 2 & Myles, Mr. Thomas Francis & male & 62 & 0 & 0 & 240276 & 9.6875 &  & Q\\\\\n",
       "\t4 & 895 & 3 & Wirz, Mr. Albert & male & 27 & 0 & 0 & 315154 & 8.6625 &  & S\\\\\n",
       "\t5 & 896 & 3 & Hirvonen, Mrs. Alexander (Helga E Lindqvist) & female & 22 & 1 & 1 & 3101298 & 12.2875 &  & S\\\\\n",
       "\t6 & 897 & 3 & Svensson, Mr. Johan Cervin & male & 14 & 0 & 0 & 7538 & 9.225 &  & S\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  PassengerId Pclass                                         Name    Sex  Age\n",
       "1         892      3                             Kelly, Mr. James   male 34.5\n",
       "2         893      3             Wilkes, Mrs. James (Ellen Needs) female 47.0\n",
       "3         894      2                    Myles, Mr. Thomas Francis   male 62.0\n",
       "4         895      3                             Wirz, Mr. Albert   male 27.0\n",
       "5         896      3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female 22.0\n",
       "6         897      3                   Svensson, Mr. Johan Cervin   male 14.0\n",
       "  SibSp Parch  Ticket    Fare Cabin Embarked\n",
       "1     0     0  330911  7.8292              Q\n",
       "2     1     0  363272  7.0000              S\n",
       "3     0     0  240276  9.6875              Q\n",
       "4     0     0  315154  8.6625              S\n",
       "5     1     1 3101298 12.2875              S\n",
       "6     0     0    7538  9.2250              S"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testImpute <- predict(preProcData, test)\n",
    "head(testImpute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: rJava\n",
      "Loading required package: bartMachineJARs\n",
      "Loading required package: car\n",
      "Loading required package: randomForest\n",
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "Loading required package: missForest\n",
      "Loading required package: foreach\n",
      "Loading required package: itertools\n",
      "Loading required package: iterators\n",
      "Welcome to bartMachine v1.2.2! You have 3.82GB memory available.\n",
      "\n",
      "If you run out of memory, restart R, and use e.g.\n",
      "'options(java.parameters = \"-Xmx5g\")' for 5GB of RAM before you call\n",
      "'library(bartMachine)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options(java.parameters=\"-Xmx4g\")\n",
    "library(bartMachine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bartMachine CV try: k: 2 m: 50 \n",
      ".....\n",
      "  bartMachine CV try: k: 2 m: 200 \n",
      ".....\n",
      "  bartMachine CV try: k: 3 m: 50 \n",
      ".....\n",
      "  bartMachine CV try: k: 3 m: 200 \n",
      ".....\n",
      "  bartMachine CV try: k: 5 m: 50 \n",
      ".....\n",
      "  bartMachine CV try: k: 5 m: 200 \n",
      ".....\n",
      "  bartMachine CV win: k: 2 m: 50 \n",
      "bartMachine initializing with 50 trees...\n",
      "bartMachine vars checked...\n",
      "bartMachine java init...\n",
      "bartMachine factors created...\n",
      "bartMachine before preprocess...\n",
      "bartMachine after preprocess... 13 total features...\n",
      "bartMachine sigsq estimated...\n",
      "bartMachine training data finalized...\n",
      "Now building bartMachine for classification ...Covariate importance prior ON. \n",
      "evaluating in sample data...done\n"
     ]
    }
   ],
   "source": [
    "bart_machine <- bartMachineCV(trainImpute[,-c(2,4,9,11)], trainImpute[,2],\n",
    "                            verbose = TRUE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bartMachine v1.2.2 for classification\n",
      "\n",
      "training data n = 891 and p = 12 \n",
      "built in 2.3 secs on 1 core, 50 trees, 250 burn-in and 1000 post. samples\n",
      "\n",
      "confusion matrix:\n",
      "\n",
      "           predicted 0 predicted 1 model errors\n",
      "actual 0        45.000     504.000        0.918\n",
      "actual 1       249.000      93.000        0.728\n",
      "use errors       0.847       0.844        0.845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary(bart_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds <- predict(bart_machine, new_data = testing[,predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for bartMachine {bartMachine}\"><tr><td>bartMachine {bartMachine}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Build a BART Model</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Builds a BART model for regression or classification.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "bartMachine(X = NULL, y = NULL, Xy = NULL, \n",
       "num_trees = 50, \n",
       "num_burn_in = 250, \n",
       "num_iterations_after_burn_in = 1000, \n",
       "alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, \n",
       "prob_rule_class = 0.5, \n",
       "mh_prob_steps = c(2.5, 2.5, 4)/9,\n",
       "debug_log = FALSE, \n",
       "run_in_sample = TRUE,  \n",
       "s_sq_y = \"mse\",\n",
       "sig_sq_est = NULL,\n",
       "cov_prior_vec = NULL, \n",
       "use_missing_data = FALSE, \n",
       "covariates_to_permute = NULL,\n",
       "num_rand_samps_in_library = 10000, \n",
       "use_missing_data_dummies_as_covars = FALSE, \n",
       "replace_missing_data_with_x_j_bar = FALSE,\n",
       "impute_missingness_with_rf_impute = FALSE,\n",
       "impute_missingness_with_x_j_bar_for_lm = TRUE,\n",
       "mem_cache_for_speed = TRUE,\n",
       "serialize = FALSE,\n",
       "seed = NULL,\n",
       "verbose = TRUE)\n",
       "\n",
       "build_bart_machine(X = NULL, y = NULL, Xy = NULL, \n",
       "num_trees = 50, \n",
       "num_burn_in = 250, \n",
       "num_iterations_after_burn_in = 1000, \n",
       "alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, \n",
       "prob_rule_class = 0.5, \n",
       "mh_prob_steps = c(2.5, 2.5, 4)/9,\n",
       "debug_log = FALSE, \n",
       "run_in_sample = TRUE,  \n",
       "s_sq_y = \"mse\",\n",
       "sig_sq_est = NULL,\n",
       "cov_prior_vec = NULL, \n",
       "use_missing_data = FALSE, \n",
       "covariates_to_permute = NULL,\n",
       "num_rand_samps_in_library = 10000, \n",
       "use_missing_data_dummies_as_covars = FALSE, \n",
       "replace_missing_data_with_x_j_bar = FALSE,\n",
       "impute_missingness_with_rf_impute = FALSE,\n",
       "impute_missingness_with_x_j_bar_for_lm = TRUE,\n",
       "mem_cache_for_speed = TRUE,\n",
       "serialize = FALSE,\n",
       "seed = NULL,\n",
       "verbose = TRUE)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>X</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Data frame of predictors. Factors are automatically converted to dummies internally. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Vector of response variable. If <code>y</code> is <code>numeric</code> or <code>integer</code>, a BART model for regression is built. If <code>y</code> is a factor with two levels, a BART model for classification is built.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>Xy</code></td>\n",
       "<td>\n",
       "\n",
       "<p>A data frame of predictors and the response. The response column must be named &ldquo;y&rdquo;. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>num_trees</code></td>\n",
       "<td>\n",
       "\n",
       "<p>The number of trees to be grown in the sum-of-trees model.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>num_burn_in</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Number of MCMC samples to be discarded as &ldquo;burn-in&rdquo;.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>num_iterations_after_burn_in</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Number of MCMC samples to draw from the posterior distribution of <i>\\hat{f}(x)</i>. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>alpha</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Base hyperparameter in tree prior for whether a node is nonterminal or not.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>beta</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Power hyperparameter in tree prior for whether a node is nonterminal or not.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>k</code></td>\n",
       "<td>\n",
       "\n",
       "<p>For regression, <code>k</code> determines the prior probability that <i>E(Y|X)</i> is contained in the interval <i>(y_{min}, y_{max})</i>, based on a normal distribution. For example, when <i>k=2</i>, the prior probability is 95%. For classification, <code>k</code> determines the prior probability that <i>E(Y|X)</i> is between <i>(-3,3)</i>. Note that a larger value of <code>k</code> results in more shrinkage and a more conservative fit. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>q</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Quantile of the prior on the error variance at which the data-based estimate is placed. Note that the larger the value of <code>q</code>, the more aggressive the fit as you are placing more prior weight on values lower than the data-based estimate. Not used for classification.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nu</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Degrees of freedom for the inverse <i>&chi;^2</i> prior. Not used for classification.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>prob_rule_class</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Threshold for classification. Any observation with a conditional probability greater than <code>prob_class_rule</code> is assigned the &ldquo;positive&rdquo; outcome. Note that the first level of the response is treated as the &ldquo;negative&rdquo; outcome and the second is treated as the &ldquo;positive&rdquo; outcome.  \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>mh_prob_steps</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Vector of prior probabilities for proposing changes to the tree structures: (GROW, PRUNE, CHANGE)\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>debug_log</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If TRUE, additional information about the model construction are printed to a file in the working directory.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>run_in_sample</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If TRUE, in-sample statistics such as <i>\\hat{f}(x)</i>, Pseudo-<i>R^2</i>, and RMSE are computed. Setting this to FALSE when not needed can decrease computation time. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>s_sq_y</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If &ldquo;mse&rdquo;, a data-based estimated of the error variance is computed as the MSE from ordinary least squares regression. If &ldquo;var&rdquo;., the data-based estimate is computed as the variance of the response. Not used in classification. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>sig_sq_est</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Pass in an estimate of the maximum sig_sq of the model. This is useful to cache somewhere and then pass in during cross-validation since the default method of estimation is a linear model. In large dimensions, linear model estimation is slow.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cov_prior_vec</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Vector assigning relative weights to how often a particular variable should be proposed as a candidate for a split. The vector is internally normalized so that the weights sum to 1. Note that the length of this vector must equal the length of the design matrix after dummification and augmentation of indicators of missingness (if used). To see what the dummified matrix looks like, use <code>dummify_data</code>. See Bleich et al. (2013) for more details on when this feature is most appropriate. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>use_missing_data</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If TRUE, the missing data feature is used to automatically handle missing data without imputation. See Kapelner and Bleich (2013) for details. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>covariates_to_permute</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Private argument for <code>cov_importance_test</code>. Not needed by user. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>num_rand_samps_in_library</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Before building a BART model, samples from the Standard Normal and <i>&chi;^2(&nu;)</i> are drawn to be used in the MCMC steps. This parameter determines the number of samples to be taken.  \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>use_missing_data_dummies_as_covars</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If TRUE, additional indicator variables for whether or not an observation in a particular column is missing are included. See Kapelner and Bleich (2013) for details.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>replace_missing_data_with_x_j_bar</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If TRUE ,missing entries in <code>X</code> are imputed with average value or modal category.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>impute_missingness_with_rf_impute</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If TRUE, missing entries are filled in using the rf.impute() function from the <code>randomForest</code> library. \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>impute_missingness_with_x_j_bar_for_lm</code></td>\n",
       "<td>\n",
       "\n",
       "<p>If TRUE, when computing the data-based estimate of <i>&sigma;^2</i>, missing entries are imputed with average value or modal category.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>mem_cache_for_speed</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Speed enhancement that caches the predictors and the split values that are available at each node for selecting new rules. If the number\n",
       "of predictors is large, the memory requirements become large. We recommend keeping this on (default) and turning it off if you experience out-of-memory errors.  \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>serialize</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Setting this option to <code>TRUE</code> will allow serialization of bartMachine objects which allows for persistence between\n",
       "R sessions if the object is saved and reloaded. Note that serialized objects can take up a large amount of memory. \n",
       "Thus, the default is <code>FALSE</code>.  \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>seed</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Optional: sets the seed in both R and Java. Default is <code>NULL</code> which does not set the seed in R nor Java.  \n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>verbose</code></td>\n",
       "<td>\n",
       "\n",
       "<p>Prints information about progress of the algorithm to the screen. \n",
       "</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>Returns an object of class &ldquo;bartMachine&rdquo;. The &ldquo;bartMachine&rdquo; object contains a list of the following components:\n",
       "</p>\n",
       "<table summary=\"R valueblock\">\n",
       "<tr valign=\"top\"><td><code>java_bart_machine</code></td>\n",
       "<td>\n",
       "<p>A pointer to the BART Java object.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>train_data_features</code></td>\n",
       "<td>\n",
       "<p>The names of the variables used in the training data.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>training_data_features_with_missing_features.</code></td>\n",
       "<td>\n",
       "<p>The names of the variables used in the training data. If <code>use_missing_data_dummies_as_covars = TRUE</code>, this also includes dummies for any predictors that contain at least one missing entry (named &ldquo;M_&lt;feature&gt;&rdquo;).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y</code></td>\n",
       "<td>\n",
       "<p>The values of the response for the training data.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y_levels</code></td>\n",
       "<td>\n",
       "<p>The levels of the response (for classification only).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>pred_type</code></td>\n",
       "<td>\n",
       "<p>Whether the model was build for regression of classification.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>model_matrix_training_data</code></td>\n",
       "<td>\n",
       "<p>The training data with factors converted to dummies.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>num_cores</code></td>\n",
       "<td>\n",
       "<p>The number of cores used to build the BART model.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>sig_sq_est</code></td>\n",
       "<td>\n",
       "<p>The data-based estimate of <i>&sigma;^2</i> used to create the prior on the error variance for the BART model.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>time_to_build</code></td>\n",
       "<td>\n",
       "<p>Total time to build the BART model.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y_hat_train</code></td>\n",
       "<td>\n",
       "<p>The posterior means of <i>\\hat{f}(x)</i> for each observation. Only returned if <code>run_in_sample = TRUE</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>residuals</code></td>\n",
       "<td>\n",
       "<p>The model residuals given by <code>y</code> - <code>y_hat_train</code>. Only returned if <code>run_in_sample = TRUE</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>L1_err_train</code></td>\n",
       "<td>\n",
       "<p>L1 error on the training set. Only returned if <code>run_in_sample = TRUE</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>L2_err_train</code></td>\n",
       "<td>\n",
       "<p>L2 error on the training set. Only returned if <code>run_in_sample = TRUE</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>PseudoRsq</code></td>\n",
       "<td>\n",
       "<p>Calculated as 1 - SSE / SST where SSE is the sum of square errors in the training data and SST is the sample variance of the response times <i>n-1</i>. Only returned if <code>run_in_sample = TRUE</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>rmse_train</code></td>\n",
       "<td>\n",
       "<p>Root mean square error on the training set. Only returned if <code>run_in_sample = TRUE</code>.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "<p>Additionally, the parameters passed to the function <code>bartMachine</code> are also components of the list. \n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Note</h3>\n",
       "\n",
       "<p>This function is parallelized by the number of cores set by <code>set_bart_machine_num_cores</code>. Each core will create an \n",
       "independent MCMC chain of size <br />\n",
       "<code>num_burn_in</code> <i>+</i> <code>num_iterations_after_burn_in / bart_machine_num_cores</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Adam Kapelner and Justin Bleich\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning\n",
       "with Bayesian Additive Regression Trees. Journal of Statistical\n",
       "Software, 70(4), 1-40. doi:10.18637/jss.v070.i04\n",
       "</p>\n",
       "<p>HA Chipman, EI George, and RE McCulloch. BART: Bayesian Additive Regressive Trees.\n",
       "The Annals of Applied Statistics, 4(1): 266&ndash;298, 2010.\n",
       "</p>\n",
       "<p>A Kapelner and J Bleich. Prediction with Missing Data via Bayesian Additive Regression\n",
       "Trees. Canadian Journal of Statistics, 43(2): 224-239, 2015\n",
       "</p>\n",
       "<p>J Bleich, A Kapelner, ST Jensen, and EI George. Variable Selection Inference for Bayesian\n",
       "Additive Regression Trees. ArXiv e-prints, 2013.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>bartMachineCV</code>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "##regression example\n",
       "\n",
       "##generate Friedman data\n",
       "set.seed(11)\n",
       "n  = 200 \n",
       "p = 5\n",
       "X = data.frame(matrix(runif(n * p), ncol = p))\n",
       "y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)\n",
       "\n",
       "##build BART regression model\n",
       "bart_machine = bartMachine(X, y)\n",
       "summary(bart_machine)\n",
       "\n",
       "## Not run: \n",
       "##Build another BART regression model\n",
       "bart_machine = bartMachine(X,y, num_trees = 200, num_burn_in = 500,\n",
       "num_iterations_after_burn_in = 1000)\n",
       "\n",
       "##Classification example\n",
       "\n",
       "#get data and only use 2 factors\n",
       "data(iris)\n",
       "iris2 = iris[51:150,]\n",
       "iris2$Species = factor(iris2$Species)\n",
       "\n",
       "#build BART classification model\n",
       "bart_machine = build_bart_machine(iris2[ ,1:4], iris2$Species)\n",
       "\n",
       "##get estimated probabilities\n",
       "phat = bart_machine$p_hat_train\n",
       "##look at in-sample confusion matrix\n",
       "bart_machine$confusion_matrix\n",
       "\n",
       "## End(Not run)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>bartMachine</em> version 1.2.2 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{bartMachine}{Build a BART Model}{bartMachine}\n",
       "\\aliasA{build\\_bart\\_machine}{bartMachine}{build.Rul.bart.Rul.machine}\n",
       "\\keyword{\\textbackslash{}textasciitilde{}kwd1}{bartMachine}\n",
       "\\keyword{\\textbackslash{}textasciitilde{}kwd2}{bartMachine}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Builds a BART model for regression or classification.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "bartMachine(X = NULL, y = NULL, Xy = NULL, \n",
       "num_trees = 50, \n",
       "num_burn_in = 250, \n",
       "num_iterations_after_burn_in = 1000, \n",
       "alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, \n",
       "prob_rule_class = 0.5, \n",
       "mh_prob_steps = c(2.5, 2.5, 4)/9,\n",
       "debug_log = FALSE, \n",
       "run_in_sample = TRUE,  \n",
       "s_sq_y = \"mse\",\n",
       "sig_sq_est = NULL,\n",
       "cov_prior_vec = NULL, \n",
       "use_missing_data = FALSE, \n",
       "covariates_to_permute = NULL,\n",
       "num_rand_samps_in_library = 10000, \n",
       "use_missing_data_dummies_as_covars = FALSE, \n",
       "replace_missing_data_with_x_j_bar = FALSE,\n",
       "impute_missingness_with_rf_impute = FALSE,\n",
       "impute_missingness_with_x_j_bar_for_lm = TRUE,\n",
       "mem_cache_for_speed = TRUE,\n",
       "serialize = FALSE,\n",
       "seed = NULL,\n",
       "verbose = TRUE)\n",
       "\n",
       "build_bart_machine(X = NULL, y = NULL, Xy = NULL, \n",
       "num_trees = 50, \n",
       "num_burn_in = 250, \n",
       "num_iterations_after_burn_in = 1000, \n",
       "alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, \n",
       "prob_rule_class = 0.5, \n",
       "mh_prob_steps = c(2.5, 2.5, 4)/9,\n",
       "debug_log = FALSE, \n",
       "run_in_sample = TRUE,  \n",
       "s_sq_y = \"mse\",\n",
       "sig_sq_est = NULL,\n",
       "cov_prior_vec = NULL, \n",
       "use_missing_data = FALSE, \n",
       "covariates_to_permute = NULL,\n",
       "num_rand_samps_in_library = 10000, \n",
       "use_missing_data_dummies_as_covars = FALSE, \n",
       "replace_missing_data_with_x_j_bar = FALSE,\n",
       "impute_missingness_with_rf_impute = FALSE,\n",
       "impute_missingness_with_x_j_bar_for_lm = TRUE,\n",
       "mem_cache_for_speed = TRUE,\n",
       "serialize = FALSE,\n",
       "seed = NULL,\n",
       "verbose = TRUE)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{X}] \n",
       "Data frame of predictors. Factors are automatically converted to dummies internally. \n",
       "\n",
       "\\item[\\code{y}] \n",
       "Vector of response variable. If \\code{y} is \\code{numeric} or \\code{integer}, a BART model for regression is built. If \\code{y} is a factor with two levels, a BART model for classification is built.\n",
       "\n",
       "\\item[\\code{Xy}] \n",
       "A data frame of predictors and the response. The response column must be named ``y''. \n",
       "\n",
       "\\item[\\code{num\\_trees}] \n",
       "The number of trees to be grown in the sum-of-trees model.\n",
       "\n",
       "\\item[\\code{num\\_burn\\_in}] \n",
       "Number of MCMC samples to be discarded as ``burn-in''.\n",
       "\n",
       "\\item[\\code{num\\_iterations\\_after\\_burn\\_in}] \n",
       "Number of MCMC samples to draw from the posterior distribution of \\eqn{\\hat{f}(x)}{}. \n",
       "\n",
       "\\item[\\code{alpha}] \n",
       "Base hyperparameter in tree prior for whether a node is nonterminal or not.\n",
       "\n",
       "\\item[\\code{beta}] \n",
       "Power hyperparameter in tree prior for whether a node is nonterminal or not.\n",
       "\n",
       "\\item[\\code{k}] \n",
       "For regression, \\code{k} determines the prior probability that \\eqn{E(Y|X)}{} is contained in the interval \\eqn{(y_{min}, y_{max})}{}, based on a normal distribution. For example, when \\eqn{k=2}{}, the prior probability is 95\\%. For classification, \\code{k} determines the prior probability that \\eqn{E(Y|X)}{} is between \\eqn{(-3,3)}{}. Note that a larger value of \\code{k} results in more shrinkage and a more conservative fit. \n",
       "\n",
       "\\item[\\code{q}] \n",
       "Quantile of the prior on the error variance at which the data-based estimate is placed. Note that the larger the value of \\code{q}, the more aggressive the fit as you are placing more prior weight on values lower than the data-based estimate. Not used for classification.\n",
       "\n",
       "\\item[\\code{nu}] \n",
       "Degrees of freedom for the inverse \\eqn{\\chi^2}{} prior. Not used for classification.\n",
       "\n",
       "\\item[\\code{prob\\_rule\\_class}] \n",
       "Threshold for classification. Any observation with a conditional probability greater than \\code{prob\\_class\\_rule} is assigned the ``positive'' outcome. Note that the first level of the response is treated as the ``negative'' outcome and the second is treated as the ``positive'' outcome.  \n",
       "\n",
       "\\item[\\code{mh\\_prob\\_steps}] \n",
       "Vector of prior probabilities for proposing changes to the tree structures: (GROW, PRUNE, CHANGE)\n",
       "\n",
       "\\item[\\code{debug\\_log}] \n",
       "If TRUE, additional information about the model construction are printed to a file in the working directory.\n",
       "\n",
       "\\item[\\code{run\\_in\\_sample}] \n",
       "If TRUE, in-sample statistics such as \\eqn{\\hat{f}(x)}{}, Pseudo-\\eqn{R^2}{}, and RMSE are computed. Setting this to FALSE when not needed can decrease computation time. \n",
       "\n",
       "\\item[\\code{s\\_sq\\_y}] \n",
       "If ``mse'', a data-based estimated of the error variance is computed as the MSE from ordinary least squares regression. If ``var''., the data-based estimate is computed as the variance of the response. Not used in classification. \n",
       "\n",
       "\\item[\\code{sig\\_sq\\_est}] \n",
       "Pass in an estimate of the maximum sig\\_sq of the model. This is useful to cache somewhere and then pass in during cross-validation since the default method of estimation is a linear model. In large dimensions, linear model estimation is slow.\n",
       "\n",
       "\\item[\\code{cov\\_prior\\_vec}] \n",
       "Vector assigning relative weights to how often a particular variable should be proposed as a candidate for a split. The vector is internally normalized so that the weights sum to 1. Note that the length of this vector must equal the length of the design matrix after dummification and augmentation of indicators of missingness (if used). To see what the dummified matrix looks like, use \\code{\\LinkA{dummify\\_data}{dummify.Rul.data}}. See Bleich et al. (2013) for more details on when this feature is most appropriate. \n",
       "\n",
       "\\item[\\code{use\\_missing\\_data}] \n",
       "If TRUE, the missing data feature is used to automatically handle missing data without imputation. See Kapelner and Bleich (2013) for details. \n",
       "\n",
       "\\item[\\code{covariates\\_to\\_permute}] \n",
       "Private argument for \\code{\\LinkA{cov\\_importance\\_test}{cov.Rul.importance.Rul.test}}. Not needed by user. \n",
       "\n",
       "\\item[\\code{num\\_rand\\_samps\\_in\\_library}] \n",
       "Before building a BART model, samples from the Standard Normal and \\eqn{\\chi^2(\\nu)}{} are drawn to be used in the MCMC steps. This parameter determines the number of samples to be taken.  \n",
       "\n",
       "\\item[\\code{use\\_missing\\_data\\_dummies\\_as\\_covars}] \n",
       "If TRUE, additional indicator variables for whether or not an observation in a particular column is missing are included. See Kapelner and Bleich (2013) for details.\n",
       "\n",
       "\\item[\\code{replace\\_missing\\_data\\_with\\_x\\_j\\_bar}] \n",
       "If TRUE ,missing entries in \\code{X} are imputed with average value or modal category.\n",
       "\n",
       "\n",
       "\\item[\\code{impute\\_missingness\\_with\\_rf\\_impute}] \n",
       "If TRUE, missing entries are filled in using the rf.impute() function from the \\code{randomForest} library. \n",
       "\n",
       "\\item[\\code{impute\\_missingness\\_with\\_x\\_j\\_bar\\_for\\_lm}] \n",
       "If TRUE, when computing the data-based estimate of \\eqn{\\sigma^2}{}, missing entries are imputed with average value or modal category.\n",
       "\n",
       "\\item[\\code{mem\\_cache\\_for\\_speed}] \n",
       "Speed enhancement that caches the predictors and the split values that are available at each node for selecting new rules. If the number\n",
       "of predictors is large, the memory requirements become large. We recommend keeping this on (default) and turning it off if you experience out-of-memory errors.  \n",
       "\n",
       "\\item[\\code{serialize}] \n",
       "Setting this option to \\code{TRUE} will allow serialization of bartMachine objects which allows for persistence between\n",
       "R sessions if the object is saved and reloaded. Note that serialized objects can take up a large amount of memory. \n",
       "Thus, the default is \\code{FALSE}.  \n",
       "\n",
       "\\item[\\code{seed}] \n",
       "Optional: sets the seed in both R and Java. Default is \\code{NULL} which does not set the seed in R nor Java.  \n",
       "\n",
       "\\item[\\code{verbose}] \n",
       "Prints information about progress of the algorithm to the screen. \n",
       "\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Value}\n",
       "Returns an object of class ``bartMachine''. The ``bartMachine'' object contains a list of the following components:\n",
       "\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{java\\_bart\\_machine}] A pointer to the BART Java object.\n",
       "\\item[\\code{train\\_data\\_features}] The names of the variables used in the training data.\n",
       "\\item[\\code{training\\_data\\_features\\_with\\_missing\\_features.}] The names of the variables used in the training data. If \\code{use\\_missing\\_data\\_dummies\\_as\\_covars = TRUE}, this also includes dummies for any predictors that contain at least one missing entry (named ``M\\_<feature>'').\n",
       "\\item[\\code{y}] The values of the response for the training data.\n",
       "\\item[\\code{y\\_levels}] The levels of the response (for classification only).\n",
       "\\item[\\code{pred\\_type}] Whether the model was build for regression of classification.\n",
       "\\item[\\code{model\\_matrix\\_training\\_data}] The training data with factors converted to dummies.\n",
       "\\item[\\code{num\\_cores}] The number of cores used to build the BART model.\n",
       "\\item[\\code{sig\\_sq\\_est}] The data-based estimate of \\eqn{\\sigma^2}{} used to create the prior on the error variance for the BART model.\n",
       "\\item[\\code{time\\_to\\_build}] Total time to build the BART model.\n",
       "\\item[\\code{y\\_hat\\_train}] The posterior means of \\eqn{\\hat{f}(x)}{} for each observation. Only returned if \\code{run\\_in\\_sample = TRUE}.\n",
       "\\item[\\code{residuals}] The model residuals given by \\code{y} - \\code{y\\_hat\\_train}. Only returned if \\code{run\\_in\\_sample = TRUE}.\n",
       "\\item[\\code{L1\\_err\\_train}] L1 error on the training set. Only returned if \\code{run\\_in\\_sample = TRUE}.\n",
       "\\item[\\code{L2\\_err\\_train}] L2 error on the training set. Only returned if \\code{run\\_in\\_sample = TRUE}.\n",
       "\\item[\\code{PseudoRsq}] Calculated as 1 - SSE / SST where SSE is the sum of square errors in the training data and SST is the sample variance of the response times \\eqn{n-1}{}. Only returned if \\code{run\\_in\\_sample = TRUE}.\n",
       "\\item[\\code{rmse\\_train}] Root mean square error on the training set. Only returned if \\code{run\\_in\\_sample = TRUE}.\n",
       "\n",
       "\\end{ldescription}\n",
       "Additionally, the parameters passed to the function \\code{bartMachine} are also components of the list. \n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Note}\\relax\n",
       "This function is parallelized by the number of cores set by \\code{\\LinkA{set\\_bart\\_machine\\_num\\_cores}{set.Rul.bart.Rul.machine.Rul.num.Rul.cores}}. Each core will create an \n",
       "independent MCMC chain of size \\\\{}\n",
       "\\code{num\\_burn\\_in} \\eqn{+}{} \\code{num\\_iterations\\_after\\_burn\\_in / bart\\_machine\\_num\\_cores}.\n",
       "\\end{Note}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "Adam Kapelner and Justin Bleich\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning\n",
       "with Bayesian Additive Regression Trees. Journal of Statistical\n",
       "Software, 70(4), 1-40. doi:10.18637/jss.v070.i04\n",
       "\n",
       "HA Chipman, EI George, and RE McCulloch. BART: Bayesian Additive Regressive Trees.\n",
       "The Annals of Applied Statistics, 4(1): 266--298, 2010.\n",
       "\n",
       "A Kapelner and J Bleich. Prediction with Missing Data via Bayesian Additive Regression\n",
       "Trees. Canadian Journal of Statistics, 43(2): 224-239, 2015\n",
       "\n",
       "J Bleich, A Kapelner, ST Jensen, and EI George. Variable Selection Inference for Bayesian\n",
       "Additive Regression Trees. ArXiv e-prints, 2013.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{bartMachineCV}{bartMachineCV}}\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "##regression example\n",
       "\n",
       "##generate Friedman data\n",
       "set.seed(11)\n",
       "n  = 200 \n",
       "p = 5\n",
       "X = data.frame(matrix(runif(n * p), ncol = p))\n",
       "y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)\n",
       "\n",
       "##build BART regression model\n",
       "bart_machine = bartMachine(X, y)\n",
       "summary(bart_machine)\n",
       "\n",
       "## Not run: \n",
       "##Build another BART regression model\n",
       "bart_machine = bartMachine(X,y, num_trees = 200, num_burn_in = 500,\n",
       "num_iterations_after_burn_in = 1000)\n",
       "\n",
       "##Classification example\n",
       "\n",
       "#get data and only use 2 factors\n",
       "data(iris)\n",
       "iris2 = iris[51:150,]\n",
       "iris2$Species = factor(iris2$Species)\n",
       "\n",
       "#build BART classification model\n",
       "bart_machine = build_bart_machine(iris2[ ,1:4], iris2$Species)\n",
       "\n",
       "##get estimated probabilities\n",
       "phat = bart_machine$p_hat_train\n",
       "##look at in-sample confusion matrix\n",
       "bart_machine$confusion_matrix\n",
       "\n",
       "## End(Not run)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "bartMachine            package:bartMachine             R Documentation\n",
       "\n",
       "_\bB_\bu_\bi_\bl_\bd _\ba _\bB_\bA_\bR_\bT _\bM_\bo_\bd_\be_\bl\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Builds a BART model for regression or classification.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     bartMachine(X = NULL, y = NULL, Xy = NULL, \n",
       "     num_trees = 50, \n",
       "     num_burn_in = 250, \n",
       "     num_iterations_after_burn_in = 1000, \n",
       "     alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, \n",
       "     prob_rule_class = 0.5, \n",
       "     mh_prob_steps = c(2.5, 2.5, 4)/9,\n",
       "     debug_log = FALSE, \n",
       "     run_in_sample = TRUE,  \n",
       "     s_sq_y = \"mse\",\n",
       "     sig_sq_est = NULL,\n",
       "     cov_prior_vec = NULL, \n",
       "     use_missing_data = FALSE, \n",
       "     covariates_to_permute = NULL,\n",
       "     num_rand_samps_in_library = 10000, \n",
       "     use_missing_data_dummies_as_covars = FALSE, \n",
       "     replace_missing_data_with_x_j_bar = FALSE,\n",
       "     impute_missingness_with_rf_impute = FALSE,\n",
       "     impute_missingness_with_x_j_bar_for_lm = TRUE,\n",
       "     mem_cache_for_speed = TRUE,\n",
       "     serialize = FALSE,\n",
       "     seed = NULL,\n",
       "     verbose = TRUE)\n",
       "     \n",
       "     build_bart_machine(X = NULL, y = NULL, Xy = NULL, \n",
       "     num_trees = 50, \n",
       "     num_burn_in = 250, \n",
       "     num_iterations_after_burn_in = 1000, \n",
       "     alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, \n",
       "     prob_rule_class = 0.5, \n",
       "     mh_prob_steps = c(2.5, 2.5, 4)/9,\n",
       "     debug_log = FALSE, \n",
       "     run_in_sample = TRUE,  \n",
       "     s_sq_y = \"mse\",\n",
       "     sig_sq_est = NULL,\n",
       "     cov_prior_vec = NULL, \n",
       "     use_missing_data = FALSE, \n",
       "     covariates_to_permute = NULL,\n",
       "     num_rand_samps_in_library = 10000, \n",
       "     use_missing_data_dummies_as_covars = FALSE, \n",
       "     replace_missing_data_with_x_j_bar = FALSE,\n",
       "     impute_missingness_with_rf_impute = FALSE,\n",
       "     impute_missingness_with_x_j_bar_for_lm = TRUE,\n",
       "     mem_cache_for_speed = TRUE,\n",
       "     serialize = FALSE,\n",
       "     seed = NULL,\n",
       "     verbose = TRUE)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       X: Data frame of predictors. Factors are automatically converted\n",
       "          to dummies internally.\n",
       "\n",
       "       y: Vector of response variable. If ‘y’ is ‘numeric’ or\n",
       "          ‘integer’, a BART model for regression is built. If ‘y’ is a\n",
       "          factor with two levels, a BART model for classification is\n",
       "          built.\n",
       "\n",
       "      Xy: A data frame of predictors and the response. The response\n",
       "          column must be named ``y''.\n",
       "\n",
       "num_trees: The number of trees to be grown in the sum-of-trees model.\n",
       "\n",
       "num_burn_in: Number of MCMC samples to be discarded as ``burn-in''.\n",
       "\n",
       "num_iterations_after_burn_in: Number of MCMC samples to draw from the\n",
       "          posterior distribution of \\hat{f}(x).\n",
       "\n",
       "   alpha: Base hyperparameter in tree prior for whether a node is\n",
       "          nonterminal or not.\n",
       "\n",
       "    beta: Power hyperparameter in tree prior for whether a node is\n",
       "          nonterminal or not.\n",
       "\n",
       "       k: For regression, ‘k’ determines the prior probability that\n",
       "          E(Y|X) is contained in the interval (y_{min}, y_{max}), based\n",
       "          on a normal distribution. For example, when k=2, the prior\n",
       "          probability is 95%. For classification, ‘k’ determines the\n",
       "          prior probability that E(Y|X) is between (-3,3). Note that a\n",
       "          larger value of ‘k’ results in more shrinkage and a more\n",
       "          conservative fit.\n",
       "\n",
       "       q: Quantile of the prior on the error variance at which the\n",
       "          data-based estimate is placed. Note that the larger the value\n",
       "          of ‘q’, the more aggressive the fit as you are placing more\n",
       "          prior weight on values lower than the data-based estimate.\n",
       "          Not used for classification.\n",
       "\n",
       "      nu: Degrees of freedom for the inverse chi^2 prior. Not used for\n",
       "          classification.\n",
       "\n",
       "prob_rule_class: Threshold for classification. Any observation with a\n",
       "          conditional probability greater than ‘prob_class_rule’ is\n",
       "          assigned the ``positive'' outcome. Note that the first level\n",
       "          of the response is treated as the ``negative'' outcome and\n",
       "          the second is treated as the ``positive'' outcome.\n",
       "\n",
       "mh_prob_steps: Vector of prior probabilities for proposing changes to\n",
       "          the tree structures: (GROW, PRUNE, CHANGE)\n",
       "\n",
       "debug_log: If TRUE, additional information about the model construction\n",
       "          are printed to a file in the working directory.\n",
       "\n",
       "run_in_sample: If TRUE, in-sample statistics such as \\hat{f}(x),\n",
       "          Pseudo-R^2, and RMSE are computed. Setting this to FALSE when\n",
       "          not needed can decrease computation time.\n",
       "\n",
       "  s_sq_y: If ``mse'', a data-based estimated of the error variance is\n",
       "          computed as the MSE from ordinary least squares regression.\n",
       "          If ``var''., the data-based estimate is computed as the\n",
       "          variance of the response. Not used in classification.\n",
       "\n",
       "sig_sq_est: Pass in an estimate of the maximum sig_sq of the model.\n",
       "          This is useful to cache somewhere and then pass in during\n",
       "          cross-validation since the default method of estimation is a\n",
       "          linear model. In large dimensions, linear model estimation is\n",
       "          slow.\n",
       "\n",
       "cov_prior_vec: Vector assigning relative weights to how often a\n",
       "          particular variable should be proposed as a candidate for a\n",
       "          split. The vector is internally normalized so that the\n",
       "          weights sum to 1. Note that the length of this vector must\n",
       "          equal the length of the design matrix after dummification and\n",
       "          augmentation of indicators of missingness (if used). To see\n",
       "          what the dummified matrix looks like, use ‘dummify_data’. See\n",
       "          Bleich et al. (2013) for more details on when this feature is\n",
       "          most appropriate.\n",
       "\n",
       "use_missing_data: If TRUE, the missing data feature is used to\n",
       "          automatically handle missing data without imputation. See\n",
       "          Kapelner and Bleich (2013) for details.\n",
       "\n",
       "covariates_to_permute: Private argument for ‘cov_importance_test’. Not\n",
       "          needed by user.\n",
       "\n",
       "num_rand_samps_in_library: Before building a BART model, samples from\n",
       "          the Standard Normal and chi^2(nu) are drawn to be used in the\n",
       "          MCMC steps. This parameter determines the number of samples\n",
       "          to be taken.\n",
       "\n",
       "use_missing_data_dummies_as_covars: If TRUE, additional indicator\n",
       "          variables for whether or not an observation in a particular\n",
       "          column is missing are included. See Kapelner and Bleich\n",
       "          (2013) for details.\n",
       "\n",
       "replace_missing_data_with_x_j_bar: If TRUE ,missing entries in ‘X’ are\n",
       "          imputed with average value or modal category.\n",
       "\n",
       "impute_missingness_with_rf_impute: If TRUE, missing entries are filled\n",
       "          in using the rf.impute() function from the ‘randomForest’\n",
       "          library.\n",
       "\n",
       "impute_missingness_with_x_j_bar_for_lm: If TRUE, when computing the\n",
       "          data-based estimate of sigma^2, missing entries are imputed\n",
       "          with average value or modal category.\n",
       "\n",
       "mem_cache_for_speed: Speed enhancement that caches the predictors and\n",
       "          the split values that are available at each node for\n",
       "          selecting new rules. If the number of predictors is large,\n",
       "          the memory requirements become large. We recommend keeping\n",
       "          this on (default) and turning it off if you experience\n",
       "          out-of-memory errors.\n",
       "\n",
       "serialize: Setting this option to ‘TRUE’ will allow serialization of\n",
       "          bartMachine objects which allows for persistence between R\n",
       "          sessions if the object is saved and reloaded. Note that\n",
       "          serialized objects can take up a large amount of memory.\n",
       "          Thus, the default is ‘FALSE’.\n",
       "\n",
       "    seed: Optional: sets the seed in both R and Java. Default is ‘NULL’\n",
       "          which does not set the seed in R nor Java.\n",
       "\n",
       " verbose: Prints information about progress of the algorithm to the\n",
       "          screen.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     Returns an object of class ``bartMachine''. The ``bartMachine''\n",
       "     object contains a list of the following components:\n",
       "\n",
       "java_bart_machine: A pointer to the BART Java object.\n",
       "\n",
       "train_data_features: The names of the variables used in the training\n",
       "          data.\n",
       "\n",
       "training_data_features_with_missing_features.: The names of the\n",
       "          variables used in the training data. If\n",
       "          ‘use_missing_data_dummies_as_covars = TRUE’, this also\n",
       "          includes dummies for any predictors that contain at least one\n",
       "          missing entry (named ``M_<feature>'').\n",
       "\n",
       "       y: The values of the response for the training data.\n",
       "\n",
       "y_levels: The levels of the response (for classification only).\n",
       "\n",
       "pred_type: Whether the model was build for regression of\n",
       "          classification.\n",
       "\n",
       "model_matrix_training_data: The training data with factors converted to\n",
       "          dummies.\n",
       "\n",
       "num_cores: The number of cores used to build the BART model.\n",
       "\n",
       "sig_sq_est: The data-based estimate of sigma^2 used to create the prior\n",
       "          on the error variance for the BART model.\n",
       "\n",
       "time_to_build: Total time to build the BART model.\n",
       "\n",
       "y_hat_train: The posterior means of \\hat{f}(x) for each observation.\n",
       "          Only returned if ‘run_in_sample = TRUE’.\n",
       "\n",
       "residuals: The model residuals given by ‘y’ - ‘y_hat_train’. Only\n",
       "          returned if ‘run_in_sample = TRUE’.\n",
       "\n",
       "L1_err_train: L1 error on the training set. Only returned if\n",
       "          ‘run_in_sample = TRUE’.\n",
       "\n",
       "L2_err_train: L2 error on the training set. Only returned if\n",
       "          ‘run_in_sample = TRUE’.\n",
       "\n",
       "PseudoRsq: Calculated as 1 - SSE / SST where SSE is the sum of square\n",
       "          errors in the training data and SST is the sample variance of\n",
       "          the response times n-1. Only returned if ‘run_in_sample =\n",
       "          TRUE’.\n",
       "\n",
       "rmse_train: Root mean square error on the training set. Only returned\n",
       "          if ‘run_in_sample = TRUE’.\n",
       "     Additionally, the parameters passed to the function ‘bartMachine’\n",
       "     are also components of the list.\n",
       "\n",
       "_\bN_\bo_\bt_\be:\n",
       "\n",
       "     This function is parallelized by the number of cores set by\n",
       "     ‘set_bart_machine_num_cores’. Each core will create an independent\n",
       "     MCMC chain of size\n",
       "     ‘num_burn_in’ + ‘num_iterations_after_burn_in /\n",
       "     bart_machine_num_cores’.\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Adam Kapelner and Justin Bleich\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning\n",
       "     with Bayesian Additive Regression Trees. Journal of Statistical\n",
       "     Software, 70(4), 1-40. doi:10.18637/jss.v070.i04\n",
       "\n",
       "     HA Chipman, EI George, and RE McCulloch. BART: Bayesian Additive\n",
       "     Regressive Trees. The Annals of Applied Statistics, 4(1): 266-298,\n",
       "     2010.\n",
       "\n",
       "     A Kapelner and J Bleich. Prediction with Missing Data via Bayesian\n",
       "     Additive Regression Trees. Canadian Journal of Statistics, 43(2):\n",
       "     224-239, 2015\n",
       "\n",
       "     J Bleich, A Kapelner, ST Jensen, and EI George. Variable Selection\n",
       "     Inference for Bayesian Additive Regression Trees. ArXiv e-prints,\n",
       "     2013.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     ‘bartMachineCV’\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     ##regression example\n",
       "     \n",
       "     ##generate Friedman data\n",
       "     set.seed(11)\n",
       "     n  = 200 \n",
       "     p = 5\n",
       "     X = data.frame(matrix(runif(n * p), ncol = p))\n",
       "     y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)\n",
       "     \n",
       "     ##build BART regression model\n",
       "     bart_machine = bartMachine(X, y)\n",
       "     summary(bart_machine)\n",
       "     \n",
       "     ## Not run:\n",
       "     \n",
       "     ##Build another BART regression model\n",
       "     bart_machine = bartMachine(X,y, num_trees = 200, num_burn_in = 500,\n",
       "     num_iterations_after_burn_in = 1000)\n",
       "     \n",
       "     ##Classification example\n",
       "     \n",
       "     #get data and only use 2 factors\n",
       "     data(iris)\n",
       "     iris2 = iris[51:150,]\n",
       "     iris2$Species = factor(iris2$Species)\n",
       "     \n",
       "     #build BART classification model\n",
       "     bart_machine = build_bart_machine(iris2[ ,1:4], iris2$Species)\n",
       "     \n",
       "     ##get estimated probabilities\n",
       "     phat = bart_machine$p_hat_train\n",
       "     ##look at in-sample confusion matrix\n",
       "     bart_machine$confusion_matrix\n",
       "     ## End(Not run)\n",
       "     "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?bartMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: statmod\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Your next step is to start H2O:\n",
      "    > h2o.init()\n",
      "\n",
      "For H2O package documentation, ask for help:\n",
      "    > ??h2o\n",
      "\n",
      "After starting H2O, you can use the Web UI at http://localhost:54321\n",
      "For more information visit http://docs.h2o.ai\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Attaching package: ‘h2o’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    sd, var\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    ||, &&, %*%, apply, as.factor, as.numeric, colnames, colnames<-,\n",
      "    ifelse, %in%, is.character, is.factor, is.numeric, log, log10,\n",
      "    log1p, log2, round, signif, trunc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library('h2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H2O is not running yet, starting it now...\n",
      "\n",
      "Note:  In case of errors look at the following log files:\n",
      "    /tmp/Rtmp3QUKUW/h2o_UnknownUser_started_from_r.out\n",
      "    /tmp/Rtmp3QUKUW/h2o_UnknownUser_started_from_r.err\n",
      "\n",
      "\n",
      "Starting H2O JVM and connecting: .. Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         1 seconds 859 milliseconds \n",
      "    H2O cluster version:        3.8.1.3 \n",
      "    H2O cluster name:           H2O_started_from_R_jupyter_uxw300 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   1.71 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  2 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    R Version:                  R version 3.2.3 (2015-12-10) \n",
      "\n",
      "Note:  As started, H2O is limited to the CRAN default of 2 CPUs.\n",
      "       Shut down and restart H2O as shown below to use all your CPUs.\n",
      "           > h2o.shutdown()\n",
      "           > h2o.init(nthreads = -1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PassengerId</th><th scope=col>Survived</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>6</td><td>0</td><td>3</td><td>Moran, Mr. James</td><td>male</td><td>31.87982</td><td>0</td><td>0</td><td>330877</td><td>8.4583</td><td></td><td>Q</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & PassengerId & Survived & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked\\\\\n",
       "\\hline\n",
       "\t1 & 1 & 0 & 3 & Braund, Mr. Owen Harris & male & 22 & 1 & 0 & A/5 21171 & 7.25 &  & S\\\\\n",
       "\t2 & 2 & 1 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599 & 71.2833 & C85 & C\\\\\n",
       "\t3 & 3 & 1 & 3 & Heikkinen, Miss. Laina & female & 26 & 0 & 0 & STON/O2. 3101282 & 7.925 &  & S\\\\\n",
       "\t4 & 4 & 1 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel) & female & 35 & 1 & 0 & 113803 & 53.1 & C123 & S\\\\\n",
       "\t5 & 5 & 0 & 3 & Allen, Mr. William Henry & male & 35 & 0 & 0 & 373450 & 8.05 &  & S\\\\\n",
       "\t6 & 6 & 0 & 3 & Moran, Mr. James & male & 31.87982 & 0 & 0 & 330877 & 8.4583 &  & Q\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  PassengerId Survived Pclass\n",
       "1           1        0      3\n",
       "2           2        1      1\n",
       "3           3        1      3\n",
       "4           4        1      1\n",
       "5           5        0      3\n",
       "6           6        0      3\n",
       "                                                 Name    Sex      Age SibSp\n",
       "1                             Braund, Mr. Owen Harris   male 22.00000     1\n",
       "2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.00000     1\n",
       "3                              Heikkinen, Miss. Laina female 26.00000     0\n",
       "4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.00000     1\n",
       "5                            Allen, Mr. William Henry   male 35.00000     0\n",
       "6                                    Moran, Mr. James   male 31.87982     0\n",
       "  Parch           Ticket    Fare Cabin Embarked\n",
       "1     0        A/5 21171  7.2500              S\n",
       "2     0         PC 17599 71.2833   C85        C\n",
       "3     0 STON/O2. 3101282  7.9250              S\n",
       "4     0           113803 53.1000  C123        S\n",
       "5     0           373450  8.0500              S\n",
       "6     0           330877  8.4583              Q"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.hex <- as.h2o(trainImpute)\n",
    "head(trainset.hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In .h2o.validateModelParameters(algo, param_values, h2oRestApiVersion): Dropping constant columns: [Name, Cabin, Ticket, Embarked, Sex].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |============================                                          |  40%\r",
      "  |                                                                            \r",
      "  |===============================================================       |  90%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "model.deep <- h2o.deeplearning(x = c(1,3:12), y = 2, training_frame = trainset.hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: deeplearning\n",
      "Model Key:  DeepLearning_model_R_1461773003859_1 \n",
      "Status of Neuron Layers: predicting Survived, 2-class classification, bernoulli distribution, CrossEntropy loss, 42,002 weights/biases, 500.4 KB, 8,910 training samples, mini-batch size 1\n",
      "  layer units      type dropout       l1       l2 mean_rate rate_RMS momentum\n",
      "1     1     6     Input  0.00 %                                              \n",
      "2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.005314 0.002873 0.000000\n",
      "3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.149048 0.239094 0.000000\n",
      "4     4     2   Softmax         0.000000 0.000000  0.003823 0.003659 0.000000\n",
      "  mean_weight weight_RMS mean_bias bias_RMS\n",
      "1                                          \n",
      "2    0.006220   0.086528  0.291732 0.072651\n",
      "3   -0.012477   0.070255  0.967055 0.019484\n",
      "4    0.039837   0.367579  0.000005 0.007526\n",
      "\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on training data. **\n",
      "Description: Metrics reported on full training frame\n",
      "\n",
      "MSE:  0.2054306\n",
      "R^2:  0.1313953\n",
      "LogLoss:  0.6040574\n",
      "AUC:  0.7668169\n",
      "Gini:  0.5336337\n",
      "\n",
      "Confusion Matrix for F1-optimal threshold:\n",
      "         0   1    Error      Rate\n",
      "0      381 168 0.306011  =168/549\n",
      "1       99 243 0.289474   =99/342\n",
      "Totals 480 411 0.299663  =267/891\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                      metric threshold    value idx\n",
      "1                     max f1  0.535322 0.645418 191\n",
      "2                     max f2  0.186893 0.772282 346\n",
      "3               max f0point5  0.728378 0.671115 110\n",
      "4               max accuracy  0.728378 0.738496 110\n",
      "5              max precision  0.999098 1.000000   0\n",
      "6                 max recall  0.099608 1.000000 385\n",
      "7            max specificity  0.999098 1.000000   0\n",
      "8           max absolute_MCC  0.728378 0.428346 110\n",
      "9 max min_per_class_accuracy  0.541514 0.698830 189\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "Scoring History: \n",
      "            timestamp   duration training_speed   epochs iterations     samples\n",
      "1 2016-04-27 16:17:58  0.000 sec                 0.00000          0    0.000000\n",
      "2 2016-04-27 16:17:58  1.204 sec  1390 rows/sec  1.00000          1  891.000000\n",
      "3 2016-04-27 16:18:01  3.270 sec  3363 rows/sec 10.00000         10 8910.000000\n",
      "  training_MSE training_r2 training_logloss training_AUC training_lift\n",
      "1                                                                     \n",
      "2      0.35110    -0.48451          2.25167      0.75440       2.02632\n",
      "3      0.20543     0.13140          0.60406      0.76682       2.60526\n",
      "  training_classification_error\n",
      "1                              \n",
      "2                       0.33333\n",
      "3                       0.29966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary(model.deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>381</td><td>168</td><td>0.3060109</td><td> =168/549</td></tr>\n",
       "\t<tr><th scope=row>1</th><td>99</td><td>243</td><td>0.2894737</td><td> =99/342</td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>480</td><td>411</td><td>0.2996633</td><td> =267/891</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & 0 & 1 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t0 & 381 & 168 & 0.3060109 &  =168/549\\\\\n",
       "\t1 & 99 & 243 & 0.2894737 &  =99/342\\\\\n",
       "\tTotals & 480 & 411 & 0.2996633 &  =267/891\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Confusion Matrix for max f1 @ threshold = 0.535322182708602:\n",
       "         0   1    Error      Rate\n",
       "0      381 168 0.306011  =168/549\n",
       "1       99 243 0.289474   =99/342\n",
       "Totals 480 411 0.299663  =267/891"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.confusionMatrix(model.deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PassengerId</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>892</td><td>3</td><td>Kelly, Mr. James</td><td>male</td><td>34.5</td><td>0</td><td>0</td><td>330911</td><td>7.8292</td><td></td><td>Q</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>893</td><td>3</td><td>Wilkes, Mrs. James (Ellen Needs)</td><td>female</td><td>47</td><td>1</td><td>0</td><td>363272</td><td>7</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>894</td><td>2</td><td>Myles, Mr. Thomas Francis</td><td>male</td><td>62</td><td>0</td><td>0</td><td>240276</td><td>9.6875</td><td></td><td>Q</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>895</td><td>3</td><td>Wirz, Mr. Albert</td><td>male</td><td>27</td><td>0</td><td>0</td><td>315154</td><td>8.6625</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>896</td><td>3</td><td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td><td>female</td><td>22</td><td>1</td><td>1</td><td>3101298</td><td>12.2875</td><td></td><td>S</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>897</td><td>3</td><td>Svensson, Mr. Johan Cervin</td><td>male</td><td>14</td><td>0</td><td>0</td><td>7538</td><td>9.225</td><td></td><td>S</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & PassengerId & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked\\\\\n",
       "\\hline\n",
       "\t1 & 892 & 3 & Kelly, Mr. James & male & 34.5 & 0 & 0 & 330911 & 7.8292 &  & Q\\\\\n",
       "\t2 & 893 & 3 & Wilkes, Mrs. James (Ellen Needs) & female & 47 & 1 & 0 & 363272 & 7 &  & S\\\\\n",
       "\t3 & 894 & 2 & Myles, Mr. Thomas Francis & male & 62 & 0 & 0 & 240276 & 9.6875 &  & Q\\\\\n",
       "\t4 & 895 & 3 & Wirz, Mr. Albert & male & 27 & 0 & 0 & 315154 & 8.6625 &  & S\\\\\n",
       "\t5 & 896 & 3 & Hirvonen, Mrs. Alexander (Helga E Lindqvist) & female & 22 & 1 & 1 & 3101298 & 12.2875 &  & S\\\\\n",
       "\t6 & 897 & 3 & Svensson, Mr. Johan Cervin & male & 14 & 0 & 0 & 7538 & 9.225 &  & S\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  PassengerId Pclass                                         Name    Sex  Age\n",
       "1         892      3                             Kelly, Mr. James   male 34.5\n",
       "2         893      3             Wilkes, Mrs. James (Ellen Needs) female 47.0\n",
       "3         894      2                    Myles, Mr. Thomas Francis   male 62.0\n",
       "4         895      3                             Wirz, Mr. Albert   male 27.0\n",
       "5         896      3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female 22.0\n",
       "6         897      3                   Svensson, Mr. Johan Cervin   male 14.0\n",
       "  SibSp Parch  Ticket    Fare Cabin Embarked\n",
       "1     0     0  330911  7.8292              Q\n",
       "2     1     0  363272  7.0000              S\n",
       "3     0     0  240276  9.6875              Q\n",
       "4     0     0  315154  8.6625              S\n",
       "5     1     1 3101298 12.2875              S\n",
       "6     0     0    7538  9.2250              S"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.hex <- as.h2o(testImpute)\n",
    "head(testset.hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>predict</th><th scope=col>p0</th><th scope=col>p1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td>0.9000671</td><td>0.09993289</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0</td><td>0.8823388</td><td>0.1176612</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0</td><td>0.8175698</td><td>0.1824302</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0</td><td>0.8820124</td><td>0.1179876</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0</td><td>0.6933068</td><td>0.3066932</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0</td><td>0.7122283</td><td>0.2877717</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & predict & p0 & p1\\\\\n",
       "\\hline\n",
       "\t1 & 0 & 0.9000671 & 0.09993289\\\\\n",
       "\t2 & 0 & 0.8823388 & 0.1176612\\\\\n",
       "\t3 & 0 & 0.8175698 & 0.1824302\\\\\n",
       "\t4 & 0 & 0.8820124 & 0.1179876\\\\\n",
       "\t5 & 0 & 0.6933068 & 0.3066932\\\\\n",
       "\t6 & 0 & 0.7122283 & 0.2877717\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  predict        p0         p1\n",
       "1       0 0.9000671 0.09993289\n",
       "2       0 0.8823388 0.11766125\n",
       "3       0 0.8175698 0.18243016\n",
       "4       0 0.8820124 0.11798763\n",
       "5       0 0.6933068 0.30669321\n",
       "6       0 0.7122283 0.28777172"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds <- predict(model.deep, newdata = testset.hex)\n",
    "head(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PassengerId</th><th scope=col>Survived</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>892</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>893</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>894</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>895</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>896</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>897</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PassengerId & Survived\\\\\n",
       "\\hline\n",
       "\t1 & 892 & 0\\\\\n",
       "\t2 & 893 & 0\\\\\n",
       "\t3 & 894 & 0\\\\\n",
       "\t4 & 895 & 0\\\\\n",
       "\t5 & 896 & 0\\\\\n",
       "\t6 & 897 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  PassengerId Survived\n",
       "1         892        0\n",
       "2         893        0\n",
       "3         894        0\n",
       "4         895        0\n",
       "5         896        0\n",
       "6         897        0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission <- data.frame(PassengerId = as.vector(testset.hex$PassengerId),\n",
    "                         Survived = as.vector(preds$predict),\n",
    "                         stringsAsFactors = FALSE)\n",
    "head(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write.csv(submission, 'titanic-submission.csv', row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with h2o.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyper_params <- list(\n",
    "  hidden=list(\n",
    "    c(rep(25, 2), rep(10, 2)),\n",
    "    c(rep(50, 2), rep(25, 2)),\n",
    "    c(rep(100, 4)))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |==========================                                            |  37%\r",
      "  |                                                                            \r",
      "  |============================================                          |  63%\r",
      "  |                                                                            \r",
      "  |===============================================                       |  67%\r",
      "  |                                                                            \r",
      "  |===================================================                   |  73%\r",
      "  |                                                                            \r",
      "  |======================================================                |  77%\r",
      "  |                                                                            \r",
      "  |========================================================              |  80%\r",
      "  |                                                                            \r",
      "  |==========================================================            |  83%\r",
      "  |                                                                            \r",
      "  |=============================================================         |  87%\r",
      "  |                                                                            \r",
      "  |=================================================================     |  93%\r",
      "  |                                                                            \r",
      "  |====================================================================  |  97%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "grid <- h2o.grid('deeplearning', x = c(1,3:12), y = 2, training_frame = trainset.hex,\n",
    "                  activation = \"Tanh\",\n",
    "                  autoencoder = FALSE, epochs = 20,\n",
    "                  hyper_params = hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O Grid Details\n",
      "================\n",
      "\n",
      "Grid ID: Grid_DeepLearning_trainImpute_model_R_1461773003859_4 \n",
      "Used hyper parameters: \n",
      "  -  hidden \n",
      "Number of models: 3 \n",
      "Number of failed models: 0 \n",
      "\n",
      "Hyper-Parameter Search Summary: ordered by increasing logloss\n",
      "   hidden                                                     model_ids\n",
      "1      25 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_0\n",
      "2      50 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_1\n",
      "3     100 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_2\n",
      "4      25 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_0\n",
      "5      50 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_1\n",
      "6     100 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_2\n",
      "7      10 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_0\n",
      "8      25 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_1\n",
      "9     100 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_2\n",
      "10     10 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_0\n",
      "11     25 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_1\n",
      "12    100 Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_2\n",
      "             logloss\n",
      "1  0.526464940156229\n",
      "2  0.530770189462801\n",
      "3  0.562835581418012\n",
      "4  0.526464940156229\n",
      "5  0.530770189462801\n",
      "6  0.562835581418012\n",
      "7  0.526464940156229\n",
      "8  0.530770189462801\n",
      "9  0.562835581418012\n",
      "10 0.526464940156229\n",
      "11 0.530770189462801\n",
      "12 0.562835581418012\n",
      "H2O Grid Summary\n",
      "================\n",
      "\n",
      "Grid ID: Grid_DeepLearning_trainImpute_model_R_1461773003859_4 \n",
      "Used hyper parameters: \n",
      "  -  hidden \n",
      "Number of models: 3 \n",
      "  -  Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_0 \n",
      "  -  Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_1 \n",
      "  -  Grid_DeepLearning_trainImpute_model_R_1461773003859_4_model_2 \n",
      "\n",
      "Number of failed models: 0 \n"
     ]
    }
   ],
   "source": [
    "summary(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ids <- grid@model_ids\n",
    "models <- lapply(model_ids, function(id) { h2o.getModel(id)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>362</td><td>187</td><td>0.3406193</td><td> =187/549</td></tr>\n",
       "\t<tr><th scope=row>1</th><td>78</td><td>264</td><td>0.2280702</td><td> =78/342</td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>440</td><td>451</td><td>0.2974186</td><td> =265/891</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & 0 & 1 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t0 & 362 & 187 & 0.3406193 &  =187/549\\\\\n",
       "\t1 & 78 & 264 & 0.2280702 &  =78/342\\\\\n",
       "\tTotals & 440 & 451 & 0.2974186 &  =265/891\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Confusion Matrix for max f1 @ threshold = 0.29518725314608:\n",
       "         0   1    Error      Rate\n",
       "0      362 187 0.340619  =187/549\n",
       "1       78 264 0.228070   =78/342\n",
       "Totals 440 451 0.297419  =265/891"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>373</td><td>176</td><td>0.3205829</td><td> =176/549</td></tr>\n",
       "\t<tr><th scope=row>1</th><td>81</td><td>261</td><td>0.2368421</td><td> =81/342</td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>454</td><td>437</td><td>0.28844</td><td> =257/891</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & 0 & 1 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t0 & 373 & 176 & 0.3205829 &  =176/549\\\\\n",
       "\t1 & 81 & 261 & 0.2368421 &  =81/342\\\\\n",
       "\tTotals & 454 & 437 & 0.28844 &  =257/891\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Confusion Matrix for max f1 @ threshold = 0.315261775079832:\n",
       "         0   1    Error      Rate\n",
       "0      373 176 0.320583  =176/549\n",
       "1       81 261 0.236842   =81/342\n",
       "Totals 454 437 0.288440  =257/891"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>379</td><td>170</td><td>0.3096539</td><td> =170/549</td></tr>\n",
       "\t<tr><th scope=row>1</th><td>95</td><td>247</td><td>0.2777778</td><td> =95/342</td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>474</td><td>417</td><td>0.2974186</td><td> =265/891</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & 0 & 1 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t0 & 379 & 170 & 0.3096539 &  =170/549\\\\\n",
       "\t1 & 95 & 247 & 0.2777778 &  =95/342\\\\\n",
       "\tTotals & 474 & 417 & 0.2974186 &  =265/891\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Confusion Matrix for max f1 @ threshold = 0.319738684515393:\n",
       "         0   1    Error      Rate\n",
       "0      379 170 0.309654  =170/549\n",
       "1       95 247 0.277778   =95/342\n",
       "Totals 474 417 0.297419  =265/891"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.confusionMatrix(models[[1]])\n",
    "h2o.confusionMatrix(models[[2]])\n",
    "h2o.confusionMatrix(models[[3]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
